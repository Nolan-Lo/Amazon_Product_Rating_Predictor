{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "399070a9-7fb9-4070-8052-99778026c721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.0)\n",
      "Requirement already satisfied: findspark in /home/jsease/.local/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/jsease/.local/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# 1) Environment setup\n",
    "!pip install pyspark findspark matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "244c7489-3a7f-4940-9373-1dd761344d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /scratch/jsease/job_39780153/matplotlib-r89b0794 because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, glob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import lit, length, col, count, when, isnan, avg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, length, isnan, when, count, from_unixtime, date_format, date_trunc, year, avg\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mtick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0116c7d7-b952-4171-a915-684c828b392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config('spark.executor.instances', 19) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1050d642-84d3-4900-bacf-eb96f8deb02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: review_Cell_Phones_and_Accessories.parquet\n",
      "Loaded: review_Grocery_and_Gourmet_Food.parquet\n",
      "Loaded: review_Health_and_Personal_Care.parquet\n",
      "Loaded: review_Arts_Crafts_and_Sewing.parquet\n",
      "Loaded: review_Pet_Supplies.parquet\n",
      "Loaded: review_Toys_and_Games.parquet\n",
      "Loaded: review_Industrial_and_Scientific.parquet\n",
      "Loaded: review_Patio_Lawn_and_Garden.parquet\n",
      "Loaded: review_Movies_and_TV.parquet\n",
      "Loaded: review_Gift_Cards.parquet\n",
      "Loaded: review_Video_Games.parquet\n",
      "Loaded: review_Tools_and_Home_Improvement.parquet\n",
      "Loaded: review_Office_Products.parquet\n",
      "Loaded: review_Digital_Music.parquet\n",
      "Loaded: review_Automotive.parquet\n",
      "Loaded: review_Sports_and_Outdoors.parquet\n",
      "Loaded: review_All_Beauty.parquet\n",
      "Loaded: review_Musical_Instruments.parquet\n",
      "Loaded: review_Amazon_Fashion.parquet\n",
      "Loaded: review_CDs_and_Vinyl.parquet\n",
      "Loaded: review_Kindle_Store.parquet\n",
      "Loaded: review_Magazine_Subscriptions.parquet\n",
      "Loaded: review_Beauty_and_Personal_Care.parquet\n",
      "Loaded: review_Health_and_Household.parquet\n",
      "Loaded: review_Appliances.parquet\n",
      "Loaded: review_Baby_Products.parquet\n",
      "Loaded: review_Subscription_Boxes.parquet\n",
      "Loaded: review_Software.parquet\n"
     ]
    }
   ],
   "source": [
    "# Load data in\n",
    "\n",
    "folder_path = \"/expanse/lustre/projects/uci150/mzidell/reviews_parquet\"\n",
    "all_files = os.listdir(folder_path)\n",
    "parquet_files = [f for f in all_files if f.endswith('.parquet')]\n",
    "\n",
    "all_df = []\n",
    "for file in parquet_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    category = file.strip('review_').strip('.parquet')\n",
    "    df = sc.read.parquet(file_path).withColumn(\"category\", lit(category))\n",
    "    all_df.append(df)\n",
    "    print(f\"Loaded: {file}\")\n",
    "\n",
    "reviews = reduce(DataFrame.unionByName, all_df)\n",
    "reviews = reviews.withColumn(\"text_len\", length(col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14e952d7-120f-4f01-b8ad-aa07c5848989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering and preparing features...\n",
      "Step 1 complete in 0.02 seconds\n",
      "Defining pipeline stages...\n",
      "Step 2 complete in 0.03 seconds\n",
      "Splitting data...\n",
      "Step 3 complete in 0.00 seconds\n",
      "Training model...\n",
      "Model training complete in 69.59 seconds\n",
      "Making predictions...\n",
      "Predictions complete in 0.11 seconds\n",
      "Evaluating accuracy...\n",
      "Evaluation complete in 3.70 seconds\n",
      "Test accuracy: 0.5231\n",
      "Total time elapsed: 73.45 seconds\n"
     ]
    }
   ],
   "source": [
    "# Attempting to go on the whole dataset but still undersampling \n",
    "import time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, StringIndexer, OneHotEncoder, HashingTF, IDF, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "\n",
    "# Timer start\n",
    "total_start = time.time()\n",
    "\n",
    "# Step 1: Filter and feature engineering\n",
    "print(\"Filtering and preparing features...\")\n",
    "t1 = time.time()\n",
    "\n",
    "reviews_clean = reviews.filter(col(\"rating\").isNotNull() & col(\"text\").isNotNull())\n",
    "\n",
    "fraction_to_keep = 0.4\n",
    "majority_df = df.filter(col(\"rating\") == 5)\n",
    "minority_df = df.filter(col(\"rating\") != 5)\n",
    "majority_sampled = majority_df.sample(withReplacement=False, fraction=fraction_to_keep, seed=42)\n",
    "reviews_clean = minority_df.union(majority_sampled).cache()\n",
    "\n",
    "#reviews_clean = reviews_undersampled.sample(withReplacement=False, fraction=0.01, seed=42).cache()\n",
    "\n",
    "reviews_fe = reviews_clean.selectExpr(\n",
    "    \"rating\", \"title\", \"text\", \"verified_purchase\", \"helpful_vote\", \"timestamp\", \"category\",\n",
    "    \"length(text) as text_length\",\n",
    "    \"cast(verified_purchase as double) as verified_numeric\"\n",
    ")\n",
    "\n",
    "print(f\"Step 1 complete in {time.time() - t1:.2f} seconds\")\n",
    "\n",
    "print(\"Defining pipeline stages...\")\n",
    "t2 = time.time()\n",
    "\n",
    "text_tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"text_tokens\")\n",
    "text_remover = StopWordsRemover(inputCol=\"text_tokens\", outputCol=\"text_filtered\")\n",
    "text_hashingTF = HashingTF(inputCol=\"text_filtered\", outputCol=\"text_rawFeatures\", numFeatures=10000)\n",
    "text_idf = IDF(inputCol=\"text_rawFeatures\", outputCol=\"textFeatures\")\n",
    "\n",
    "title_tokenizer = RegexTokenizer(inputCol=\"title\", outputCol=\"title_tokens\")\n",
    "title_remover = StopWordsRemover(inputCol=\"title_tokens\", outputCol=\"title_filtered\")\n",
    "title_hashingTF = HashingTF(inputCol=\"title_filtered\", outputCol=\"title_rawFeatures\", numFeatures=5000)\n",
    "title_idf = IDF(inputCol=\"title_rawFeatures\", outputCol=\"titleFeatures\")\n",
    "\n",
    "category_indexer = StringIndexer(inputCol=\"category\", outputCol=\"category_index\", handleInvalid=\"keep\")\n",
    "category_encoder = OneHotEncoder(inputCol=\"category_index\", outputCol=\"category_encoded\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"textFeatures\", \"titleFeatures\", \"text_length\", \"verified_numeric\", \"helpful_vote\", \"timestamp\", \"category_encoded\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"rating\",\n",
    "    maxIter=100,\n",
    "    regParam=0.005,      \n",
    "    elasticNetParam=0.0   \n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    text_tokenizer, text_remover, text_hashingTF, text_idf,\n",
    "    title_tokenizer, title_remover, title_hashingTF, title_idf,\n",
    "    category_indexer, category_encoder,\n",
    "    assembler,\n",
    "    lr\n",
    "])\n",
    "print(f\"Step 2 complete in {time.time() - t2:.2f} seconds\")\n",
    "\n",
    "# Step 3: Train-test split\n",
    "print(\"Splitting data...\")\n",
    "t3 = time.time()\n",
    "train_df, test_df = reviews_fe.randomSplit([0.8, 0.2], seed=42)\n",
    "train_df = train_df.repartition(100)\n",
    "test_df = test_df.repartition(50)\n",
    "print(f\"Step 3 complete in {time.time() - t3:.2f} seconds\")\n",
    "\n",
    "# Step 4: Train model\n",
    "print(\"Training model...\")\n",
    "t4 = time.time()\n",
    "model = pipeline.fit(train_df)\n",
    "print(f\"Model training complete in {time.time() - t4:.2f} seconds\")\n",
    "\n",
    "# Step 5: Make predictions\n",
    "print(\"Making predictions...\")\n",
    "t5 = time.time()\n",
    "predictions = model.transform(test_df)\n",
    "print(f\"Predictions complete in {time.time() - t5:.2f} seconds\")\n",
    "\n",
    "# Step 6: Evaluate accuracy\n",
    "print(\"Evaluating accuracy...\")\n",
    "t6 = time.time()\n",
    "\n",
    "rounded_preds = predictions.withColumn(\"rounded_prediction\", \n",
    "    when(col(\"prediction\") < 2.5, 1.0)\n",
    "   .when(col(\"prediction\") < 2.85, 2.0)\n",
    "   .when(col(\"prediction\") < 3.3, 3.0)\n",
    "   .when(col(\"prediction\") < 4.05, 4.0)\n",
    "   .otherwise(5.0)\n",
    ")\n",
    "\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"rounded_prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(rounded_preds)\n",
    "\n",
    "print(f\"Evaluation complete in {time.time() - t6:.2f} seconds\")\n",
    "\n",
    "# Final output\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "print(f\"Total time elapsed: {time.time() - total_start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79743a36-c7ff-474e-9489-025ccdeb1cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+\n",
      "|rounded_prediction| count|\n",
      "+------------------+------+\n",
      "|               1.0|136333|\n",
      "|               2.0| 45963|\n",
      "|               3.0| 87322|\n",
      "|               4.0|174213|\n",
      "|               5.0|212769|\n",
      "+------------------+------+\n",
      "\n",
      "+------+------+\n",
      "|rating| count|\n",
      "+------+------+\n",
      "|   1.0|138758|\n",
      "|   2.0| 47918|\n",
      "|   3.0| 83963|\n",
      "|   4.0|171332|\n",
      "|   5.0|214629|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rounded_preds.groupBy(\"rounded_prediction\").count().orderBy(\"rounded_prediction\").show()\n",
    "rounded_preds.groupBy(\"rating\").count().orderBy(\"rating\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
